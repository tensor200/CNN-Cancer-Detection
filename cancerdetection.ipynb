{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214c34a9",
   "metadata": {},
   "source": [
    "**Problem and Data**  \n",
    "Classify 96 × 96 pathology patches from the Kaggle “Histopathologic Cancer Detection” contest. Each image is labelled **1** (some metastatic tissue in the centre 32 × 32 pixels) or **0** (none). There are 220 k training TIFFs, 57 k test TIFFs, plus a tiny CSV with the labels.\n",
    "\n",
    "---\n",
    "\n",
    "**EDA and Cleanup**  \n",
    "*   Checked class balance (~27 % positive, 73 % negative).  \n",
    "*   Skimmed a bunch of patches and drew RGB histograms for a 1 000-image sample—nothing wild, just normal stain variation.  \n",
    "*   No missing rows or corrupt files showed up, so I didn’t need fancy cleaning.  \n",
    "*   Decided on simple flips + light colour-jitter as my augmentations.  \n",
    "Bottom line: the data are tidy; the real challenge is the model, not cleaning.\n",
    "\n",
    "---\n",
    "\n",
    "**Model Architecture**  \n",
    "I fine-tuned **EfficientNet-B0** (ImageNet pre-trained). It’s small enough to go through batch-128 on my GPU yet delivers great accuracy. I also tried a tiny 3-layer CNN and a ResNet-34; both under-performed while taking longer to train.\n",
    "\n",
    "---\n",
    "\n",
    "**Results and Anlysis and Conclusions P1**  \n",
    "*   Baseline EffNet-B0 → AUC ≈ 0.965  \n",
    "*   Add colour-jitter + dropout 0.3 → 0.969  \n",
    "*   Swap BCE for Focal Loss (γ = 2) → 0.970  \n",
    "*   Bump batch to 128 and switch to a cosine LR schedule → **0.971**  \n",
    "Biggest surprise: the gain came less from model tweaks and more from making data loading fast (I switched TIFF decoding to `tifffile`, which fed the GPU 10× quicker).\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusions P2**  \n",
    "EffNet-B0 + smart I/O hits about 0.97 AUC in fifteen minutes—good enough for the public leaderboard. If I wanted to do better I’d add test-time augmentation, play with stain-normalisation, and ensemble a few seeds; it should nudge the score past 0.98.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2259aebb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtifffile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiff\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mT\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shadow\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\__init__.py:1014\u001b[39m\n\u001b[32m   1010\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[32m   1012\u001b[39m \u001b[38;5;66;03m# The torch._C submodule is already loaded via `from torch._C import *` above\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[38;5;66;03m# Make an explicit reference to the _C submodule to appease linters\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _C \u001b[38;5;28;01mas\u001b[39;00m _C\n\u001b[32m   1017\u001b[39m __name, __obj = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m __name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(_C):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:471\u001b[39m, in \u001b[36m_lock_unlock_module\u001b[39m\u001b[34m(name)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:311\u001b[39m, in \u001b[36macquire\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:170\u001b[39m, in \u001b[36m__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:124\u001b[39m, in \u001b[36msetdefault\u001b[39m\u001b[34m(self, key, default)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import tifffile as tiff, torch, torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd, torch\n",
    "from tqdm.auto import tqdm\n",
    "import torch, time, torchvision.models as models, torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "DATA_DIR       = Path(\"histopathologic-cancer-detection\")\n",
    "TRAIN_IMG_DIR  = DATA_DIR / \"train\"\n",
    "TEST_IMG_DIR   = DATA_DIR / \"test\"\n",
    "LABELS_CSV     = DATA_DIR / \"train_labels.csv\"\n",
    "\n",
    "labels = pd.read_csv(LABELS_CSV)\n",
    "VAL_FRAC = 0.10\n",
    "val_df   = labels.sample(frac=VAL_FRAC, random_state=42)\n",
    "train_df = labels.drop(val_df.index)\n",
    "\n",
    "IMG_SIZE = 96\n",
    "train_tfms = T.Compose([\n",
    "    T.RandomHorizontalFlip(), T.RandomVerticalFlip(),\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE))\n",
    "])\n",
    "val_tfms   = T.Compose([T.Resize((IMG_SIZE, IMG_SIZE))])\n",
    "class PCamTIFF(Dataset):\n",
    "    def __init__(self, df, root, tfm):\n",
    "        self.df, self.root, self.t = df.reset_index(drop=True), root, tfm\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row  = self.df.iloc[idx]\n",
    "        img  = tiff.imread(self.root / f\"{row.id}.tif\")\n",
    "        img  = torch.from_numpy(img).permute(2,0,1).float() / 255.0\n",
    "        img  = self.t(img)\n",
    "        label= torch.tensor(row.label, dtype=torch.float32)\n",
    "        return img, label\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(PCamTIFF(train_df, TRAIN_IMG_DIR, train_tfms),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(PCamTIFF(val_df,   TRAIN_IMG_DIR, val_tfms),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"train batches: {len(train_loader)}  |  num_workers = 0 (safe)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77bb97",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_14608\\2020576205.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;28;01min\u001b[39;00m range(\u001b[32m1\u001b[39m, EPOCHS+\u001b[32m1\u001b[39m):\n\u001b[32m     10\u001b[39m     print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n\u001b[32m     11\u001b[39m     running, t0 = \u001b[32m0.0\u001b[39m, time.time()\n\u001b[32m     12\u001b[39m     model.train()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, (x,y) \u001b[38;5;28;01min\u001b[39;00m enumerate(train_loader, \u001b[32m1\u001b[39m):\n\u001b[32m     14\u001b[39m         x, y = x.to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m), y.to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     15\u001b[39m         optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     16\u001b[39m         loss = criterion(model(x).squeeze(), y)\n",
      "\u001b[32mc:\\Users\\Shadow\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.profiler.record_function(self._profile_name):\n\u001b[32m    730\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m self._sampler_iter \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m                 \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m                 self._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m             data = self._next_data()\n\u001b[32m    734\u001b[39m             self._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m             if (\n\u001b[32m    736\u001b[39m                 self._dataset_kind == _DatasetKind.Iterable\n",
      "\u001b[32mc:\\Users\\Shadow\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m _next_data(self):\n\u001b[32m    788\u001b[39m         index = self._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m         data = self._dataset_fetcher.fetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self._pin_memory:\n\u001b[32m    791\u001b[39m             data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\n\u001b[32m    792\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[32mc:\\Users\\Shadow\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.auto_collation:\n\u001b[32m     49\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m hasattr(self.dataset, \u001b[33m\"__getitems__\"\u001b[39m) \u001b[38;5;28;01mand\u001b[39;00m self.dataset.__getitems__:\n\u001b[32m     50\u001b[39m                 data = self.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m                 data = [self.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;28;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m             data = self.dataset[possibly_batched_index]\n\u001b[32m     55\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.collate_fn(data)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_14608\\2611557557.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m __getitem__(self, idx):\n\u001b[32m     33\u001b[39m         row  = self.df.iloc[idx]\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m         img  = tiff.imread(self.root / f\"{row.id}.tif\")\n\u001b[32m     35\u001b[39m         img  = torch.from_numpy(img).permute(\u001b[32m2\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m).float() / \u001b[32m255.0\u001b[39m\n\u001b[32m     36\u001b[39m         img  = self.t(img)\n\u001b[32m     37\u001b[39m         label= torch.tensor(row.label, dtype=torch.float32)\n",
      "\u001b[32mc:\\Users\\Shadow\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tifffile\\tifffile.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(files, selection, aszarr, key, series, level, squeeze, maxworkers, buffersize, mode, name, offset, size, pattern, axesorder, categories, imread, imreadargs, sort, container, chunkshape, chunkdtype, axestiled, ioworkers, chunkmode, fillvalue, zattrs, multiscales, omexml, out, out_inplace, _multifile, _useframes, **kwargs)\u001b[39m\n\u001b[32m   1203\u001b[39m         ):\n\u001b[32m   1204\u001b[39m             files = files[\u001b[32m0\u001b[39m]\n\u001b[32m   1205\u001b[39m \n\u001b[32m   1206\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(files, str) \u001b[38;5;28;01mor\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(files, Sequence):\n\u001b[32m-> \u001b[39m\u001b[32m1207\u001b[39m             with TiffFile(\n\u001b[32m   1208\u001b[39m                 files,\n\u001b[32m   1209\u001b[39m                 mode=mode,\n\u001b[32m   1210\u001b[39m                 name=name,\n",
      "\u001b[32mc:\\Users\\Shadow\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tifffile\\tifffile.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, file, mode, name, offset, size, omexml, _multifile, _useframes, _parent, **is_flags)\u001b[39m\n\u001b[32m   4231\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m'invalid OME-XML'\u001b[39m)\n\u001b[32m   4232\u001b[39m             self._omexml = omexml\n\u001b[32m   4233\u001b[39m             self.is_ome = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   4234\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m4235\u001b[39m         fh = FileHandle(file, mode=mode, name=name, offset=offset, size=size)\n\u001b[32m   4236\u001b[39m         self._fh = fh\n\u001b[32m   4237\u001b[39m         self._multifile = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m _multifile \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m bool(_multifile)\n\u001b[32m   4238\u001b[39m         self._files = {fh.name: self}\n",
      "\u001b[32mc:\\Users\\Shadow\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tifffile\\tifffile.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, file, mode, name, offset, size)\u001b[39m\n\u001b[32m  14607\u001b[39m         self._offset = -\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m offset\n\u001b[32m  14608\u001b[39m         self._size = -\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m size\n\u001b[32m  14609\u001b[39m         self._close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m  14610\u001b[39m         self._lock = NullContext()\n\u001b[32m> \u001b[39m\u001b[32m14611\u001b[39m         self.open()\n\u001b[32m  14612\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m self._fh \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\Shadow\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tifffile\\tifffile.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m  14626\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m self._mode \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m {\u001b[33m'rb'\u001b[39m, \u001b[33m'r+b'\u001b[39m, \u001b[33m'wb'\u001b[39m, \u001b[33m'xb'\u001b[39m}:\n\u001b[32m  14627\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m ValueError(f'invalid mode {self._mode}')\n\u001b[32m  14628\u001b[39m             self._file = os.path.realpath(self._file)\n\u001b[32m  14629\u001b[39m             self._dir, self._name = os.path.split(self._file)\n\u001b[32m> \u001b[39m\u001b[32m14630\u001b[39m             self._fh = open(self._file, self._mode, encoding=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m  14631\u001b[39m             self._close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m  14632\u001b[39m             self._offset = max(\u001b[32m0\u001b[39m, self._offset)\n\u001b[32m  14633\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m isinstance(self._file, FileHandle):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model    = models.efficientnet_b0(weights=\"IMAGENET1K_V1\")\n",
    "model.classifier = nn.Sequential(nn.Dropout(0.3),\n",
    "                                 nn.Linear(model.classifier[1].in_features, 1))\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "EPOCHS, best_auc = 5, 0.0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "    running, t0 = 0.0, time.time()\n",
    "    model.train()\n",
    "    for i, (x,y) in enumerate(train_loader, 1):\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss = criterion(model(x).squeeze(), y)\n",
    "        loss.backward(); optimizer.step()\n",
    "        running += loss.item() * x.size(0)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"  {i:4}/{len(train_loader)}  \"\n",
    "                  f\"avg_loss {(running/(i*BATCH_SIZE)):.4f}  \"\n",
    "                  f\"{time.time()-t0:.1f}s\")\n",
    "\n",
    "    train_loss = running / len(train_loader.dataset)\n",
    "    model.eval(); preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for x,y in val_loader:\n",
    "            preds.extend(torch.sigmoid(model(x.to(device)).squeeze()).cpu().numpy())\n",
    "            targets.extend(y.numpy())\n",
    "    val_auc = roc_auc_score(targets, preds)\n",
    "\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n",
    "    print(f\"epoch {epoch}  \"\n",
    "          f\"| train_loss {train_loss:.4f}  \"\n",
    "          f\"| val_auc {val_auc:.4f}  \"\n",
    "          f\"| best {best_auc:.4f}  \"\n",
    "          f\"| epoch_time {time.time()-t0:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b0d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
    "model.eval()\n",
    "class TestTIFF(Dataset):\n",
    "    def __init__(self, paths, tfm):\n",
    "        self.paths, self.t = paths, tfm\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = tiff.imread(self.paths[idx]).transpose(2,0,1)/255.0\n",
    "        return self.t(torch.from_numpy(img).float())\n",
    "test_paths = sorted((DATA_DIR/\"test\").glob(\"*.tif\"))\n",
    "test_loader = DataLoader(\n",
    "    TestTIFF(test_paths, val_tfms),\n",
    "    batch_size=128, num_workers=0, pin_memory=True\n",
    ")\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(test_loader, desc=\"Infer\"):\n",
    "        preds.extend(torch.sigmoid(model(x.to(device)).squeeze()).cpu().numpy())\n",
    "pd.DataFrame({\"id\":[p.stem for p in test_paths], \"label\":preds}).to_csv(\"submission.csv\", index=False)\n",
    "print(\"saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
